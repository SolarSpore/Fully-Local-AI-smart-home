# Fully-Local-AI-smart-home
This project is a fully local AI-powered voice assistant, "Larry", built using Home Assistant, Whisper, Piper, and Ollama. Unlike cloud-based assistants like Alexa, everything runs locally, ensuring privacy and control over your smart home.

This project is a fully local AI-powered voice assistant built using Home Assistant, Whisper, Piper, and Ollama. Unlike cloud-based assistants such as Alexa, everything runs locally, ensuring privacy and giving me full control over my smart home. The assistant can understand spoken commands, process them through a local LLM (LLaMA 3.2 via Ollama/AMA), and respond through a local speaker. The hardware setup includes a Raspberry Pi 5 as the primary device running Home Assistant and Piper for text-to-speech, a SeeedStudio 4-mic array for voice input, and a USB speaker for output. Whisper runs in a Docker container on a separate Linux machine to handle speech-to-text. This distributed setup allows audio capture, transcription, response generation, and audio playback entirely locally.

I installed Home Assistant on the Raspberry Pi 5 and configured it with the Assist Microphone add-on, selecting the SeeedStudio mic array as the input device and the USB speaker for output. Piper remains running on the Pi for text-to-speech. On a separate Linux machine, I set up Whisper inside a Docker container to handle speech-to-text and ensured it is network-accessible from Home Assistant. I then integrated Ollama in Home Assistant, pointing it to my local AMA server running LLaMA 3.2, which serves as the assistant’s brain. In the Home Assistant voice assistant settings, I configured the speech-to-text service to Whisper, the text-to-speech service to Piper, and the conversation agent to Ollama, with Open Wake Word configured to detect my custom wake word, “Larry.” This setup creates a fully local audio pipeline where the assistant captures audio, transcribes it, generates a response, and outputs speech through Piper.

To interact with the assistant, I simply say the wake word “Larry,” which triggers the SeeedStudio mic array to capture my voice. The audio is sent to Whisper running in a Docker container on the Linux server for transcription and then forwarded to Ollama for generating a response or executing Home Assistant commands. The response is sent back to Piper on the Raspberry Pi 5, which converts it to audio and plays it through the USB speaker. I can ask questions, control lights and other smart devices, or have a conversation with the LLM, all while keeping my data completely local.

During setup, I encountered several challenges. Ollama initially would not configure properly, which I resolved by changing its IP binding to `0.0.0.0` so all PCs on the network could reach the server. The voice assistant also failed to respond until I selected “Larry” in the Assist Devices dropdown. Additionally, my NAS drive occasionally disconnected due to mounting issues, which required troubleshooting the mount configuration. Latency between transcription and response was another challenge, which I addressed by optimizing the Docker networking and audio pipeline. Throughout this process, I learned the importance of careful network configuration, distributed system debugging, and the nuances of running multiple AI services locally.

Key commands I used include running Ollama in the background with `nohup ollama serve &` or via systemd, restarting it with `sudo systemctl restart ollama`, and running Piper and Whisper in Docker containers. I also monitor GPU usage during LLM operation using `watch -n 0.5 nvidia-smi`. Docker commands are configured to ensure persistent data storage and network accessibility, allowing the assistant to run reliably across devices.
